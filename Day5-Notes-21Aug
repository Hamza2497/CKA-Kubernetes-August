

# kubectl get nodes

All the nodes have to be in ready status

# kubectl delete deployment --all

# kubectl delete pod --all

# kubectl delete pvc --all

# kubectl delete pv --all


# sudo systemctl restart nfs-kernel-server



# vim pv-mysql.yml


apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-nfs
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /data
    server: <privateipaddressofMasternode>

# kubectl create -f pv-mysql.yml
# kubectl get pv

======================================

# vim pvc-mysql.yml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mysql-nfs
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi

# kubectl create -f pvc-mysql.yml

# kubectl get pvc
==========================================
Create an Object of volume type SECRET in kubernetes that will preserve password in an Encrypted format

# vim mysqlsecret.yml

kind: Secret
apiVersion: v1
metadata:
 name: mysql-pwd
data:
 password: "cGFzc3dvcmQ="

# kubectl create -f mysqlsecret.yml

# kubectl get secrets

==================================
Create configMap on the terminal 

#  kubectl create configmap db1 --from-literal=MYSQL_DATABASE=wordpress

# kubectl get configmap

===========================================

# vim mysql-deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql
spec:
 replicas: 1
 selector:
  matchLabels:
   app: mysql-wordpress
 template:
  metadata:
   labels:
    app: mysql-wordpress
    product: mysql
  spec:
   containers:
     - name: mysql-container
       image: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         valueFrom:
          secretKeyRef:
           name: mysql-pwd
           key: password
       - name: MYSQL_DATABASE
         valueFrom:
          configMapKeyRef:
           name: db1
           key: MYSQL_DATABASE
       volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
   volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-nfs


# kubectl create -f mysql-deployment.yml

# kubectl get pods

# kubectl get all
=========================================================================

# vim service-mysql.yml

apiVersion: v1
kind: Service
metadata:
 name: mysql
spec:
 type: ClusterIP
 ports:
  - targetPort: 3306
    port: 3306
 selector:
  app: mysql-wordpress
  product: mysql

# kubectl create -f service-mysql.yml

# kubectl get all

===============================================================


Deploy wordpress:
====================

# vim wordpress-deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: wordpress
spec:
 replicas: 1
 selector:
  matchLabels:
   app: mysql-wordpress
   tier: frontend
 template:
  metadata:
   labels:
    app: mysql-wordpress
    tier: frontend
  spec:
   containers:
    - name: wordpress-container
      image: wordpress
      env:
      - name: WORDPRESS_DB_HOST
        value: mysql
      - name: WORDPRESS_DB_USER
        value: root
      - name: WORDPRESS_DB_PASSWORD
        valueFrom:
         secretKeyRef:
          name: mysql-pwd
          key: password
      
---
apiVersion: v1
kind: Service
metadata:
 name: wordpress
spec:
 type: NodePort
 ports:
  - targetPort: 80
    port: 80
 selector:
  app: mysql-wordpress
  tier: frontend

# kubectl create -f wordpress-deployment.yml

# kubectl get all
======================================================
Execute below commands and take a complete screenshot:

# kubectl get pv

# kubectl get pvc

# kubectl get secrets

# kubectl get configmap

# kubectl get all

===============================================================================

Deploy the dashboard and take a screenshot

# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
# kubectl get all  -n kubernetes-dashboard

# kubectl edit svc -n kubernetes-dashboard kubernetes-dashboard

press i

Chnage the type  in spec from ClusterIP to NodePort

Save the file

Verifying the changes

kubectl get svc -n kubernetes-dashboard -o wide

Accessing Kubernetes Dashboard and take a screenshot

Click on the master tab on the lab, and then click on the desktop option.
Open Firefox browser 
https://localhost:<<NodePort>>
Example: https://localhost:31851

Click on Advanced -> Accept Risk and Continue

=====================================

To access the dashboard we need a token for which we have to create a Service account

vim serviceaccount.yml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: sandry
  namespace: kubernetes-dashboard


# kubectl create -f serviceaccount.yml

Take a screenshot

# vim clusterrole-sa.yml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
 - kind: ServiceAccount
   name: sandry
   namespace: kubernetes-dashboard



# kubectl create -f clusterrole-sa.yml



# vim sercret-sa.yml

apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
 name: mysecret-sa
 namespace: kubernetes-dashboard
 annotations:
  kubernetes.io/service-account.name: sandry


# kubectl create -f sercret-sa.yml

# kubectl describe secret  mysecret-sa -n kubernetes-dashboard

Copy the token and then paste on the dashboard page to loginto it.

You will be able to see the kubernetes dashboard now.

Take a screenshot of the dashboard and submit the project








Scheduling in kubernetes using:
****************

NodeName:

Create a yaml file as given below : 
In the spec section of pod, write the node name where you want to scehdule the pod

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeName: gke-cluster-1-default-pool-e18bdb3b-3h5s
      containers:
      - image: leaddevops/kubeserve:v1
        name: app

save the file and execute the yaml

you will see both the pod is scheduled on your mentioned node

Node Selector:
************************

# kubectl delete deployment --all

$ kubectl get nodes

every node has multiple labels:

$ kubectl describe node <node Name>


Add your custom labels to all the nodes:

we will add label as  disk=hdd ===>  to node 1 and node 3
we will add lable as  disk=ssd ===> to node 2

$ kubectl label node <nodename1> disk=hdd

$ kubectl label node <nodename3> disk=hdd

$ kubectl label node <nodename2> disk=ssd
kubectl label node gke-cluster-1-default-pool-e18bdb3b-3l9r disk=ssd

Now create a yaml file to deploy pods with node selector

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeSelector:
       disk: ssd
      containers:
      - image: leaddevops/kubeserve:v1
        name: app


================================================

Create a replica Set, pods may be scheduled on worker1 and worker2

apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: myrs
spec:
 replicas: 3
 selector:
  matchLabels:
   type: webserver
 template:
  metadata:
   labels:
    type: webserver
  spec:
   containers:
    - name: c1
      image: nginx


On the master node taint one of the node - worker1

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule

This node will be tainted

Scale up the replicas, no pod will be scehduled on the tainted node.

# kubectl scale replicaset myrs --replicas=6

This is tainting. 

Now remove the taint:

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule-


We have 2 types of effect in taint

NoSchedule
NoExecute

Example of NoExecute : in this case the running pods on the node that is tainted will leave the node and get created somewhere else
No new pod will be scheduled on the taineted node as No execute.

Now taint one of the node as :

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoExecute

You will observe that pods form tainted node are terminated


Tolerations:
*********************

first taint a node with a key and value

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve1
spec:
  replicas: 5
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      tolerations:
        - key: color
          operator: "Equal"
          value: "red"
          effect: "NoSchedule"
      containers:
      - image: leaddevops/kubeserve:v1
        name: app
