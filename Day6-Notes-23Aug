A ConfigMap is an API object used to store non-confidential data in key-value pairs.
A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.
A ConfigMap is not designed to hold large chunks of data

For example:
==================


1. Create a properties file with key and values for dev Env
2. Create a properties file with key and values for Prod Env
3. Store the dev file in a volume of type configMap
4. Store the Prod  file in a volume of type configMap
5. Create Pod of nginx Image and on the container mount the volume of Dev ConfigMap
6. Create Pod of nginx Image and on the container mount the volume of Prod ConfigMap
Observation:
2 pods are created form the same Image
Each pod is using a different configuration
in this way we will not have to create multiple Images for each env.. Same Image will work in all the environment
we just have to mount the correct configMAP




CONFIG MAP
===========================================

# kubectl delete all --all

# vim dev.properties
app.env:dev
app.mem=2048m
app.properties=dev.env.url
:wq!

# kubectl create configmap dev-config1 --from-file=dev.properties
# kubectl get configmap

vim pod-configmap.yml

kind: Pod
apiVersion: v1
metadata:
 name: pod-configmap
spec:
 containers:
  - image: nginx
    name: c1
    volumeMounts:
     - name: config-volume
       mountPath: /etc/config
 volumes:
  - name: config-volume
    configMap:
     name: dev-config1
 restartPolicy: Never
 
 :wq!
 
 # kubectl apply -f pod-configmap.yml
 # kubectl exec -it pod-configmap -- bash
 # cd /etc/config
 
 you will find the dev.properties file and configurations
 
 Edit the configMAP
 
 kubectl edit configmap -n <namespace> <configMapName> -o yaml

This opens up a vim editor with the configmap in yaml format. Now simply edit it and save it.

Scheduling in kubernetes using:
****************

NodeName:

Create a yaml file as given below : 
In the spec section of pod, write the node name where you want to scehdule the pod

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeName: gke-cluster-1-default-pool-e18bdb3b-3h5s
      containers:
      - image: leaddevops/kubeserve:v1
        name: app

save the file and execute the yaml

you will see both the pod is scheduled on your mentioned node

Node Selector:
************************

# kubectl delete deployment --all

$ kubectl get nodes

every node has multiple labels:

$ kubectl describe node <node Name>


Add your custom labels to all the nodes:

we will add label as  disk=hdd ===>  to node 1 and node 3
we will add lable as  disk=ssd ===> to node 2

$ kubectl label node <nodename1> disk=hdd

$ kubectl label node <nodename3> disk=hdd

$ kubectl label node <nodename2> disk=ssd
kubectl label node gke-cluster-1-default-pool-e18bdb3b-3l9r disk=ssd

Now create a yaml file to deploy pods with node selector

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeSelector:
       disk: ssd
      containers:
      - image: leaddevops/kubeserve:v1
        name: app


================================================

Create a replica Set, pods may be scheduled on worker1 and worker2

apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: myrs
spec:
 replicas: 3
 selector:
  matchLabels:
   type: webserver
 template:
  metadata:
   labels:
    type: webserver
  spec:
   containers:
    - name: c1
      image: nginx


On the master node taint one of the node - worker1

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule

This node will be tainted

Scale up the replicas, no pod will be scehduled on the tainted node.

# kubectl scale replicaset myrs --replicas=6

This is tainting. 

Now remove the taint:

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule-


We have 2 types of effect in taint

NoSchedule
NoExecute

Example of NoExecute : in this case the running pods on the node that is tainted will leave the node and get created somewhere else
No new pod will be scheduled on the taineted node as No execute.

Now taint one of the node as :

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoExecute

You will observe that pods form tainted node are terminated


Tolerations:
*********************

first taint a node with a key and value

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoSchedule

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve1
spec:
  replicas: 5
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      tolerations:
        - key: color
          operator: "Equal"
          value: "red"
          effect: "NoSchedule"
      containers:
      - image: leaddevops/kubeserve:v1
        name: app
